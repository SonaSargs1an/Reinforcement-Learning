# ğŸš— Mountain Car â€” Eligibility Traces (ET)

A professional, researchâ€‘grade implementation of the **Mountain Car** control problem using **Eligibility Traces (ET)** and advanced **Temporal-Difference (TD)** learning algorithms.
This module is part of the main **Reinforcement-Learning** repository and focuses on achieving fast, stable learning in a sparseâ€‘reward environment through TD(Î») methods.



## ğŸŒ„ Overview

The **Mountain Car** task is a classic benchmark used to evaluate RL algorithms in continuous-state, lowâ€‘reward, and momentumâ€‘dependent settings. The agent controls a weak car trying to climb a steep hill, but:

* The engine cannot drive directly up the hill.
* Reward is given **only when the car reaches the goal**.
* The agent must learn to build momentum by oscillating left/right.

This makes it a perfect environment to explore more advanced temporalâ€‘credit assignment techniques.

This project implements:

* **SARSA(Î»)** and **Q(Î»)** with eligibility traces
* **Semi-gradient TD(Î»)** for function approximation
* **Efficient ET decay mechanisms**
* **Modular and extensible training pipeline**



## âš™ï¸ Key Features

### ğŸ”¹ Eligibility Traces (ET)

Eligibility traces allow the agent to keep a *decaying memory* of recent states and actions. This enables faster, more correct propagation of delayed reward signals.

### ğŸ”¹ TD(Î») Hybridization

TD methods learn from bootstrapping, while Monte Carlo learns from complete returns. TD(Î»):

* Combines both
* Smoothly controls the creditâ€‘assignment horizon
* Accelerates convergence in sparse tasks like Mountain Car

### ğŸ”¹ Semi-Gradient Methods

For continuous state spaces, the project uses:

* Linear value-function approximation
* Gradient-based updates
* Feature encoding (tile coding or custom featurizers)

This improves generalization and allows learning from large state spaces.



## ğŸ“ Folder Structure

```
mountain-car-et/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent.py              # ET-based SARSA(Î») / Q(Î») agents
â”‚   â”œâ”€â”€ env_wrapper.py         # MountainCar-v0 preprocessing & normalization
â”‚   â”œâ”€â”€ et_traces.py           # Eligibility trace data structures
â”‚   â”œâ”€â”€ approximator.py        # Feature functions and semi-gradient updates
â”‚   â”œâ”€â”€ train.py               # Training pipeline
â”‚   â”œâ”€â”€ evaluate.py            # Evaluation and trajectory rollout
â”‚   â””â”€â”€ utils.py               # Plotting, logging, helpers
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ learning_curves/       # Reward vs episodes plots
â”‚   â”œâ”€â”€ trajectories/          # Sample agent trajectories
â”‚   â””â”€â”€ configs/               # Hyperparameter logs
â”‚
â””â”€â”€ README.md
```



## ğŸ§  How Eligibility Traces Work

Eligibility traces store *how recently* each stateâ€“action pair was visited. Formally:

```
z(s, a) = Î³Î» z(s, a) + 1
```

Updates become:

```
Q â† Q + Î± Î´ z
```

where Î´ is the TD error.

This makes TD(Î»):

* Faster than Monte Carlo
* More stable than TD(0)
* Much more sample-efficient



## ğŸ§© Applications

This module is ideal for:

* Research experiments with TD(Î»)
* Comparing SARSA(Î»), Q(Î») and TD(0)
* Studying credit assignment in sparse-reward tasks
* RL coursework and academic demonstrations
* Extensions into continuous control with function approximation


