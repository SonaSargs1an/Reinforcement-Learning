ğŸŒŒ Random Walk NTD (Neural TD)

ğŸ§  A **Reinforcement Learning** implementation of **Neural Temporal Difference (NTD)** for the classic **Random Walk** environment.

---

ğŸ“˜ Table of Contents

- [ğŸ” Overview](#-overview)  
- [ğŸŒ Environment](#-environment)  
- [âš™ï¸ Algorithm](#ï¸-algorithm)  
- [ğŸ’» Installation](#-installation)  
- [ğŸš€ Usage](#-usage)  
- [ğŸ“‚ Code Structure](#-code-structure)  
- [ğŸ“Š Experiments & Results](#-experiments--results)  
- [ğŸ¤ Contributing](#-contributing)  
- [ğŸ“„ License](#-license)

---

ğŸ” Overview

**Random Walk NTD** implements neural temporal-difference learning (TD) on a simple 1D random walk MDP.  
This repository demonstrates how to:

âœ… Use neural networks to approximate value functions  
âœ… Perform online TD(0) updates  
âœ… Evaluate convergence and learning stability  

---

ğŸŒ Environment

The environment simulates a **1D random walk** problem:

- ğŸ§© **States:** discrete positions on a line (0 â†’ N)  
- ğŸ¯ **Start position:** middle of the line  
- ğŸ **Terminal states:** left and right ends  
- ğŸ’° **Reward:** +1 for right end, 0 (or âˆ’1) for left end  
- ğŸ”„ **Transitions:** each step randomly moves left or right (p=0.5)

> ğŸ§ª This environment is a standard benchmark for testing **value function learning** algorithms.

---

âš™ï¸ Algorithm

The **Neural Temporal Difference (NTD)** algorithm works as follows:

1. ğŸ§± **Initialize** a neural network \( V(s; \theta) \) with random weights \(\theta\).  
2. ğŸ” For each episode or time step:
   - Observe current state \( s \)  
   - Move left or right â†’ next state \( s' \)  
   - Receive reward \( r \)  
   - Compute TD error:  
     \[
     \delta = r + \gamma V(s'; \theta) - V(s; \theta)
     \]
   - Update weights via gradient descent:
     \[
     \theta \leftarrow \theta + \alpha \, \delta \, \nabla_\theta V(s; \theta)
     \]
3. ğŸ”š Repeat until convergence or stopping criteria.

ğŸ“Œ Parameters:
- `Î³` â€” discount factor  
- `Î±` â€” learning rate  

---


